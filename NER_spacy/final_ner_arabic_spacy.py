# -*- coding: utf-8 -*-
"""final_ner_arabic_spacy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H-h5yy-D3yxwHMQSc7HFWUgdv3eSWmcE
"""

from google.colab import drive
drive.mount('/content/drive')

"""Import libraries"""

#Download packages
!python3 -m spacy download en
!python3 -m spacy download en_core_web_sm

#Import libraries
import json 
import spacy
import random
import en_core_web_sm
from spacy import displacy

"""Load training data"""

TRAIN_DATA_EXAMPLE = [
    ('Who is Elon Musk?', {
        'entities': [(7, 15, 'PERSON')]
    }),
    ('I like London and Berlin.', {
        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]
    })
]

# Load the training data
example_path='/content/drive/MyDrive/training_data1.json'
with open(example_path) as fp:
     training_data = json.load(fp)

training_data

"""Pretrained model setup

"""

#Load pretrained model loaded from spacy
nlp_pretrained = en_core_web_sm.load()
print("Loaded model : %s" % nlp_pretrained)

#Retrieve the existing ner component
ner_pretrained=nlp.get_pipe('ner')
#Print exisiting pipeline components
print("Pipeline objects: %s" % nlp_pretrained.pipe_names)



"""Blank model setup

"""

# Prepare an empty model to train
nlp_blank = spacy.blank('en') 
# Assign name to the component
nlp_blank.vocab.vectors.name = 'demo' 
print("Loaded model : %s" % nlp_blank)

#Create a new component 'ner' to the factory
ner_blank= nlp_blank.create_pipe('ner')
#Component ner is added last to the pipeline
nlp_blank.add_pipe(ner_blank, last = True)
#Print exisiting pipeline components
print("Pipeline objects: %s" % nlp_blank.pipe_names)

"""Add custom Named Entity labels: Blank model"""

# Existing entity labels for the blank model
blank_previous_ents = ner_blank.labels 
print('[Blank model: existing entities]')
if (len(ner_blank.labels))==0:
  print("No Named Entity labels")
for name in ner_blank.labels:
  print(name)

# Add new Named Entity labels to the NER pipeline for the blank model
for label in training_data["classes"]:
  ner_blank.add_label(label)

# Updated entity labels for the pretrained model
blank_new_ents = ner_blank.labels 
print('[Blank model: new entities]')
for name in ner_blank.labels:
  print(name)

"""Add custom Named Entity labels: Pretrained model"""

# Existing entity labels for the model
pretrained_previous_ents = ner_pretrained.labels
print('[Pretrained model: existing entities]')
for name in ner_pretrained.labels[::-1]: 
  print(name) # print starting from last element

#Description of the Named Entity label
spacy.explain("GPE")

# Add new Named Entity labels to the NER pipeline for the blank model
for label in training_data["classes"]:
  ner_pretrained.add_label(label)

# Updated entity labels for the pretrained model
pretrained_new_ents = ner_pretrained.labels
print('[Pretrained model: new entities]')
for name in ner_pretrained.labels[::-1]: 
  print(name) # print starting from last element

"""Create an optimizer


"""

#Create a new optimizer for the blank model
optimizer_blank = nlp_blank.begin_training()
#Train the existing optimizer for the pretrained model
optimizer_pretrained = nlp_pretrained.resume_training()

"""Train the blank and pretrained models"""

#Disable other pipelines to train only the ner pipeline
other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
with nlp.disable_pipes(*other_pipes):  # only train NER component
#Iterate through several instances to have more training data
  for iter in range(50):
    losses={}
    random.shuffle(training_data["annotations"])
    #For each text and annotations
    for text, annotations in training_data["annotations"]:
      #Skip the segments of the text that were not annotated--skip empty entities
        if len(text) > 0:
          # Update model parameters with text and annotations for that text
          # Text, annotations are required and optimizer,losses are optional parameters
            nlp_blank.update([text], [annotations], sgd=optimizer_blank, losses=losses)
            nlp_pretrained.update([text], [annotations], sgd=optimizer_pretrained, losses=losses)
            print(losses)

#which labels have more/smaller loss

"""Load the test data"""

#read the combined txt file as input file (same as training)
test_txt = open("/content/drive/MyDrive/abbas_bandar.txt")
text=test_txt.read()
text=text.replace("\n", " ")

text

len(text)

"""Test the blank model"""

# Named Entity Recognition of the test data for the blank model
doc = nlp_blank(text)
# Display entities
doc.ents

#Display Named Entities with their names and labels for the blank model
for ent in doc.ents:
    print(ent.text,ent.label_)
    print()

# Named Entity Recognition of the test data for the blank model
doc = nlp_pretrained(text)
# Display entities
doc.ents

#Display Named Entities with their names and labels for the pretrained model
for ent in doc_pretrained.ents:
    print(ent.text,ent.label_)
    print()

"""End of the pipeline

Default ner pipeline for en_core_web_sm.load

Future: 'catastrophic forgetting problem'

https://spacy.io/api/language#resume_training

https://github.com/explosion/spaCy/issues/2124
"""

# Load English tokenizer, tagger, parser and NER
nlp = en_core_web_sm.load()
#Load the en_core_web library and create the nlp pipeline
ner=nlp.get_pipe('ner')
#Read the test text
test_txt = open("/content/drive/MyDrive/abbas_bandar.txt")
text=test_txt.read()
text=text.replace("\n", " ")
#Process the text and recognize Named Entities
doc = nlp(text)

#Visualize the NER
displacy.render(doc, style="ent", jupyter=True)

#Print all entities
for ent in doc.ents:
    print(ent.text,ent.label_)

#Print only GPE, LOC, ORG entities
for ent in doc.ents:
    if (ent.label_=="GPE" or ent.label_=="ORG" or ent.label_=="LOC" ):
      print(ent.text,ent.label_)

for ent in doc.ents:
    print(ent.text,ent.label_)